{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepQLearning-Pytorch-SpaceInvaders.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubencg195/Pytorch-Tutorials/blob/master/DeepQLearning_Pytorch_SpaceInvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zK9yKwBcMMQY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Imports\n",
        "\n",
        "https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "1IFW5W87weYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhc6iOZzYdSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bbdca8f5-3338-46a9-ac4e-49a1a77bf378"
      },
      "cell_type": "code",
      "source": [
        "OS_Windows = False\n",
        "\n",
        "if OS_Windows:\n",
        "    !ECHO %CurrentDir%\n",
        "    drive_path = \"./\"\n",
        "else:\n",
        "    #Install Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    drive_path = \"gdrive/My Drive/Colab Notebooks/\"\n",
        "    !ls  \"gdrive/My Drive/Colab Notebooks/\"\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            " Beginner\t\t\t\t\t    KRAGGLE\n",
            " Books\t\t\t\t\t\t    log_deepq_pytorch.txt\n",
            "'Convolutionary Network Tutorial - Pytorch.ipynb'   model_deepq_pytorch.ckpt\n",
            "'Copy of DeepQLearning-Pytorch.ipynb'\t\t    MUN\n",
            "'DATA LOADING AND PROCESSING TUTORIAL.ipynb'\t    PONG\n",
            " DeepQLearning-Pytorch-SpaceInvaders.ipynb\t    PyTorch\n",
            "'DeepQ Learning Tutorial.ipynb'\t\t\t    STATISTICS\n",
            "'DEEPQ NETWORK SPACE INVADERS.ipynb'\t\t    Theory\n",
            " Intermidiate\t\t\t\t\t    Udacity\n",
            "'Keras Tutorial'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DKsS_afGwtJL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "931d1ebd-684c-4aa7-fe61-76538bac4e6c"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58904000 @  0x7f2174d8f2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QgxjOBwBwvCs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#import torchvision.transforms as T\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-r-64kjMReY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "luCNGmE-JZnh",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "3a043725-ef58-445c-dd9d-23ece3ccbbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "cell_type": "code",
      "source": [
        "#Environment\n",
        "env_name = 'SpaceInvaders-v0' #@param [\"DoomBasic-v0\", \"Breakout-v0\", \"Pong-v0\"] {allow-input: true}\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "print(\"Action Space: \", env.action_space.n)\n",
        "print(\"Action Space: \", env.observation_space.shape)\n",
        "\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist()) \n",
        "print(\"Possible Actions: \\n\", possible_actions)\n",
        "\n",
        "\n",
        "### MODEL HYPERPARAMETERS\n",
        "frame_width  = 84                #@param {type:\"number\"} \n",
        "frame_height = 110               #@param {type:\"number\"} \n",
        "\n",
        "### PREPROCESSING HYPERPARAMETERS\n",
        "# Number of frames stacked\n",
        "stack_size = 4                   #@param {type:\"number\"}              \n",
        "state_size = [frame_height, frame_width, stack_size]        # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels)\n",
        "\n",
        "# 6 possible actions\n",
        "##action_size = env.action_space.n   #@param {type:\"raw\"}\n",
        "action_size = 6   #@param {type:\"raw\"}\n",
        "\n",
        "# Alpha (aka learning rate)\n",
        "learning_rate =  0.00025         #@param {type:\"number\"}      \n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "\n",
        "# Total episodes for training\n",
        "total_episodes = 20000              #@param {type:\"number\"}\n",
        "# Max possible steps in an episode\n",
        "max_steps = 50000                #@param {type:\"number\"} \n",
        "# Batch size\n",
        "batch_size = 10                #@param {type:\"number\"}              \n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "\n",
        "# exploration probability at start\n",
        "explore_start = 1.0              #@param {type:\"number\"}\n",
        "# minimum exploration probability \n",
        "explore_stop = 0.01              #@param {type:\"number\"}\n",
        "# exponential decay rate for exploration prob\n",
        "decay_rate = 0.000005             #@param {type:\"number\"}         \n",
        "\n",
        "# Q learning hyperparameters\n",
        "# Discounting rate\n",
        "gamma = 0.9                      #@param {type:\"number\"}                \n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "\n",
        "# Number of experiences stored in the Memory when initialized for the first time\n",
        "pretrain_length = batch_size  \n",
        "# Number of experiences the Memory can keep\n",
        "memory_size = 50000            #@param {type:\"number\"}       \n",
        "\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True                 #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False           #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "# Here we create an hot encoded version of our actions\n",
        "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
        "\n",
        "# We stack 4 frames\n",
        "stack_size = 4  #@param {type:\"number\"}       \n",
        "\n",
        "target_update = 50 #@param {type:\"number\"}       \n",
        "\n",
        "load_weigths  = False            #@param [\"False\", \"True\"] {type:\"raw\"}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Action Space:  6\n",
            "Action Space:  (210, 160, 3)\n",
            "Possible Actions: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cuqUsfeF28vm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "9PUHiC9jO_Fu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stack frames**\n",
        "\n",
        "As explained in this really good article we stack frames.\n",
        "\n",
        "https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
        "\n",
        "Stacking frames is really important because it helps us to give have a sense of motion to our Neural Network.\n",
        "\n",
        "BUT, we don't stack each frames, we skip 4 frames at each timestep. This means that only every fourth frame is considered. And then, we use this frame to form the stack_frame.\n",
        "\n",
        "The frame skipping method is already implemented in the library.\n",
        "\n",
        "First we preprocess frame\n",
        "Then we append the frame to the deque that automatically removes the oldest frame\n",
        "Finally we build the stacked state\n",
        "This is how work stack:\n",
        "\n",
        "For the first frame, we feed 4 frames\n",
        "At each timestep, we add the new frame to deque and then we stack them to form a new stacked frame\n",
        "And so on stack\n",
        "If we're done, we create a new stack with 4 new frames (because we are in a new episode)."
      ]
    },
    {
      "metadata": {
        "id": "IhmIM3ty2-R-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skimage import transform      # Help us to preprocess the frames\n",
        "from skimage.color import rgb2gray # Help us to gray our frames\n",
        "import matplotlib.pyplot as plt    # Display graphs\n",
        "import warnings                    # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def preprocess_frame(frame, display=False):\n",
        "  \"\"\"\n",
        "  Reduce complexity\n",
        "  Steps:\n",
        "  1. Grayscale\n",
        "  2. Crop\n",
        "  3. Normalize \n",
        "  4. Resize\n",
        "  \"\"\" \n",
        "  gray       = rgb2gray(frame)\n",
        "  cropped    = gray[8:-12, 4:-12] \n",
        "  normalized = cropped/255.0 \n",
        "  resized    = transform.resize(normalized, [110,84])\n",
        "  \n",
        "  if display: \n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=5, sharey=True, sharex=True, figsize=(25,50))\n",
        "\n",
        "    plt.subplot(1, 5, 1)\n",
        "    plt.imshow(frame)\n",
        "    \n",
        "    plt.subplot(1, 5, 2)\n",
        "    plt.imshow(gray)\n",
        "    \n",
        "    plt.subplot(1, 5, 3)\n",
        "    plt.imshow(cropped)\n",
        "    \n",
        "    plt.subplot(1, 5, 4)\n",
        "    plt.imshow(normalized)\n",
        "    \n",
        "        \n",
        "    plt.subplot(1, 5, 5)\n",
        "    plt.imshow(resized)\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    #print(\"New  Shape: \", resized.shape)\n",
        "\n",
        "  return resized # 110x84x1 frame\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_smLEsFO3qI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "\n",
        "        #print(\"stacked_frames shape\", len(stacked_frames), stacked_frames[0].shape )\n",
        "        #print(\"stacked_state shape\", stacked_state.shape)\n",
        "        #Output \n",
        "        #stacked_frames shape 4 (110, 84)\n",
        "        #stacked_state shape (110, 84, 4)\n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WjcvjrB6gqt_",
        "colab_type": "code",
        "outputId": "a386cc57-5001-4e15-cf74-182aa564bf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "print(\"Shape\",  obs.shape )\n",
        "preprocess_frame(obs, display=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape (210, 160, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZkAAAFfCAYAAAAszcLlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzJIkSZIkSZKkQZHRDgAAAAdJREFUof0/uswKQTO5fjgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4ba86d2c18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00151918, 0.00151918, 0.00151918, ..., 0.00147554, 0.        ,\n",
              "        0.        ],\n",
              "       [0.00136726, 0.00136726, 0.00029298, ..., 0.00132799, 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.00115   , 0.00115   ,\n",
              "        0.00115   ],\n",
              "       [0.000805  , 0.000805  , 0.00116643, ..., 0.001265  , 0.001265  ,\n",
              "        0.001265  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "2eQ0MQlhw7JO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Deep Q Network"
      ]
    },
    {
      "metadata": {
        "id": "iOGL5AYs1vWz",
        "colab_type": "code",
        "outputId": "c0350d8f-5251-49f1-e77d-07a3dc46d35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "def outputSize(in_size, kernel_size, stride, padding):\n",
        "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
        "    return (output)\n",
        "\n",
        "#Input (3, 110, 84) Conv, Output specified = 18 for batch size. for Maxpool layer number 1 remains unchanged\n",
        "print(\"Convolution #1\", (32,\n",
        "       outputSize(in_size=110, kernel_size=8, stride=4, padding=1 ),\n",
        "       outputSize(in_size=84 , kernel_size=8, stride=4, padding=1 ),\n",
        "      ))\n",
        "\n",
        "#Input (32, 27, 20) Conv, Output specified = 18 for batch size. for Maxpool layer number 1 remains unchanged\n",
        "print(\"Convolution #2\", (64,\n",
        "       outputSize(in_size=27 , kernel_size=4, stride=2, padding=0 ),\n",
        "       outputSize(in_size=20 , kernel_size=4, stride=2, padding=0 ),\n",
        "      ))\n",
        "\n",
        "#Input (64, 12, 9) Conv, Output specified = 18 for batch size. for Maxpool layer number 1 remains unchanged\n",
        "print(\"Convolution #3\", (128,\n",
        "       outputSize(in_size=12, kernel_size=3, stride=2, padding=0 ),\n",
        "       outputSize(in_size=9 , kernel_size=3, stride=2, padding=0 ),\n",
        "      ))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convolution #1 (32, 27, 20)\n",
            "Convolution #2 (64, 12, 9)\n",
            "Convolution #3 (128, 5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tIXD2ymqw89u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeepQ(nn.Module):\n",
        "  def __init__(self, learning_rate, num_actions, input_shape ):\n",
        "    super(DeepQ, self).__init__()\n",
        "    self.conv1  = nn.Conv2d(4,   32 , kernel_size=8, stride=4, padding=1 )\n",
        "    self.bn1    = nn.BatchNorm2d(32)\n",
        "    \n",
        "    self.conv2  = nn.Conv2d(32,   64 , kernel_size=4, stride=2, padding=0 )\n",
        "    self.bn2    = nn.BatchNorm2d(64)\n",
        "    \n",
        "    self.conv3  = nn.Conv2d(64,   128, kernel_size=3, stride=2, padding=0 )\n",
        "    self.bn3    = nn.BatchNorm2d(128)\n",
        "    \n",
        "    self.fc1    = nn.Linear(128 * 5 * 4  , 512)\n",
        "    self.fc2    = nn.Linear(512          , num_actions)\n",
        "    \n",
        "    self.optim  = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    self.loss   = nn.MSELoss()\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "    \n",
        "  def forward(self, obs):\n",
        "    #print(\"Forward Input Shape\", obs.shape)\n",
        "    obs     =obs.to(self.device)\n",
        "    obs     = obs.view(-1, 4, 110, 84 )\n",
        "    obs     = F.relu(self.bn1(self.conv1(obs)))\n",
        "    obs     = F.relu(self.bn2(self.conv2(obs)))\n",
        "    obs     = F.relu(self.bn3(self.conv3(obs)))\n",
        "    obs     = obs.view(-1, 128 * 5 * 4 )\n",
        "    obs     = F.relu(self.fc1(obs))\n",
        "    \n",
        "    actions = self.fc2(obs) #No Relu, raw data represents Q(s, a) value\n",
        "    return actions\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LEF8qS-f5VL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Memory"
      ]
    },
    {
      "metadata": {
        "id": "N1yQnioNFcDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import deque# Ordered collection with ends\n",
        "class Memory():\n",
        "  def __init__(self, max_size):\n",
        "    self.buffer = deque(maxlen = max_size)\n",
        "    \n",
        "  def add(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "   \n",
        "  def len(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "#     print('Buffer Size', len(self.buffer) )\n",
        "    return random.sample(self.buffer, batch_size)\n",
        "#     buffer_size = len(self.buffer)\n",
        "#     index       = np.random.choice(\n",
        "#         np.arange(buffer_size), \n",
        "#         size    = batch_size,\n",
        "#         replace = False \n",
        "#     )\n",
        "#     return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n718lFeM18xc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Agent"
      ]
    },
    {
      "metadata": {
        "id": "chWxq6Ni17zx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "  def __init__(self, \n",
        "                  stack_size,\n",
        "                  state_size,\n",
        "                  action_size,\n",
        "                  learning_rate,\n",
        "                  total_episodes,\n",
        "                  max_steps,\n",
        "                  batch_size,\n",
        "                  explore_start,\n",
        "                  explore_stop,\n",
        "                  decay_rate,\n",
        "                  gamma,\n",
        "                  pretrain_length,\n",
        "                  memory_size,\n",
        "                  training ,\n",
        "                  episode_render,\n",
        "                  possible_actions,\n",
        "                  target_update\n",
        "              ):\n",
        "    \n",
        "    self.stack_size         = stack_size\n",
        "    self.state_size         = state_size\n",
        "    self.action_size        = action_size\n",
        "    self.learning_rate      = learning_rate\n",
        "    self.total_episodes     = total_episodes\n",
        "    self.max_steps          = max_steps\n",
        "    self.batch_size         = batch_size\n",
        "    self.explore_start      = explore_start\n",
        "    self.explore_stop       = explore_stop\n",
        "    self.decay_rate         = decay_rate\n",
        "    self.gamma              = gamma\n",
        "    self.pretrain_length    = pretrain_length\n",
        "    self.memory_size        = memory_size\n",
        "    self.training           = training\n",
        "    self.episode_render     = episode_render\n",
        "    self.possible_actions   = possible_actions\n",
        "    self.target_update      = target_update\n",
        "    self.Q_eval             = DeepQ(\n",
        "        learning_rate       =  self.gamma\n",
        "        ,num_actions        =  self.action_size\n",
        "        ,input_shape        =  self.state_size\n",
        "    )\n",
        "    self.Q_target           = DeepQ(\n",
        "        learning_rate       =  self.gamma,\n",
        "        num_actions         =  self.action_size,\n",
        "        input_shape         =  self.state_size\n",
        "    )  \n",
        "     \n",
        "    if load_weigths != None and load_weigths == True:\n",
        "      print(\"Loadinig Models\")      \n",
        "      self.Q_target.load_state_dict(\n",
        "          T.load(\n",
        "              drive_path+\"/model_deepq_pytorch.ckpt\", \n",
        "              map_location=self.Q_target.device\n",
        "          )\n",
        "      )  \n",
        "      self.Q_target.load_state_dict(\n",
        "          T.load(\n",
        "              drive_path+\"/model_deepq_pytorch.ckpt\", \n",
        "              map_location=self.Q_target.device\n",
        "          )\n",
        "      )\n",
        "    else:\n",
        "      print(\"Loading New Weights\")\n",
        "    \n",
        "    \n",
        "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "    self.Q_eval.to(device)\n",
        "    self.Q_target.to(device)\n",
        "    \n",
        "\n",
        "    #Target Policy\n",
        "    self.memory = Memory(max_size = self.memory_size)\n",
        "    # Initialize deque with zero-images one array for each image\n",
        "    self.stacked_frames  =  deque(\n",
        "        [np.zeros((110,84), dtype=np.int) for i in range(stack_size)], \n",
        "        maxlen=4\n",
        "    )\n",
        "\n",
        "    \n",
        "    print(\n",
        "      '_'*10 +  'HYPERPARAMETERS'  +   '_'*10  + '\\n'     ,\n",
        "      '\\nstack_size        '      ,self.stack_size        , \n",
        "      '\\nstate_size        '      ,self.state_size        ,\n",
        "      '\\naction_size       '      ,self.action_size       ,\n",
        "      '\\nlearning_rate     '      ,self.learning_rate     ,\n",
        "      '\\ntotal_episodes    '      ,self.total_episodes    ,\n",
        "      '\\nmax_steps         '      ,self.max_steps         ,\n",
        "      '\\nbatch_size        '      ,self.batch_size        ,\n",
        "      '\\nexplore_start     '      ,self.explore_start     ,\n",
        "      '\\nexplore_stop      '      ,self.explore_stop      ,\n",
        "      '\\ndecay_rate        '      ,self.decay_rate        ,\n",
        "      '\\ngamma             '      ,self.gamma             ,\n",
        "      '\\npretrain_length   '      ,self.pretrain_length   ,\n",
        "      '\\nmemory_size       '      ,self.memory_size       ,\n",
        "      '\\ntraining          '      ,self.training          ,\n",
        "      '\\nepisode_render    '      ,self.episode_render    ,\n",
        "      '\\ntarget_update     '      ,self.target_update     ,\n",
        "      '\\npossible_actions\\n'      ,self.possible_actions  ,\n",
        "      '\\n'\n",
        "    )\n",
        "  \n",
        "  def save_log(self, episode,total_reward, explore_probability, loss, running_loss , time, step, exp, running, action ):\n",
        "    with open(drive_path+\"/log_deepq_pytorch.txt\" , \"a\") as myfile:\n",
        "      log = '{}\\tEpisode:\\t{}\\tTotalReward:\\t{}\\tExploreP:\\t{:.4f}\\tRunningLoss:\\t{:.4f}\\tLoss:\\t{:.4f}\\tTimeLapse:\\t{}\\tMin.\\tStep:\\t{}\\t{}\\tAction\\t{}'.format( \n",
        "          running, episode,total_reward, explore_probability, running_loss , loss , time, step, exp, action\n",
        "      ) \n",
        "      myfile.write(log)\n",
        "      print(log)\n",
        "      \n",
        "  def predict_action(self, decay_step, state):\n",
        "    #EPSILON GREEDY STRATEGY\n",
        "    #Choose action a from state s using epsilon greedy\n",
        "    #First we randomize a number\n",
        "    exp_exp_tradeoff       = np.random.rand()\n",
        "    explore_probability    = self.explore_stop         \\\n",
        "    + (self.explore_start  - self.explore_stop  )      \\\n",
        "    * np.exp(-decay_rate   * decay_step    )\n",
        "    \n",
        "    if(explore_probability > exp_exp_tradeoff):\n",
        "      #Make a random action (Exploration)\n",
        "      choice               = random.randint(1, len(self.possible_actions)) - 1\n",
        "      action               = self.possible_actions[choice]\n",
        "\n",
        "    else:\n",
        "      #Get action from QNet (Explotation)\n",
        "      action   =  self.Q_eval.forward(T.Tensor(state))\n",
        "      action   =  T.argmax(action).item()\n",
        "      \n",
        "    return action, explore_probability, (explore_probability > exp_exp_tradeoff)\n",
        "  \n",
        "  def train(self):\n",
        "    #Initialize the decay_rate (This will reduce epsilon)\n",
        "    self.decay_step        = 0\n",
        "    self.start_time        = time.time()\n",
        "    \n",
        "    self.state             = env.reset()\n",
        "    preprocess_frame(self.state, display=True)\n",
        "    \n",
        "    self.reward_list       = []\n",
        "    \n",
        "    for episode in range(self.total_episodes):\n",
        "      #Set Step to 0\n",
        "      self.step            = 0\n",
        "      \n",
        "\n",
        "      #Initialize the rewards of the episode\n",
        "      self.episode_rewards = []\n",
        "      self.episode_loss    = []\n",
        "      self.episode_actions = []\n",
        "      \n",
        "      #Make a new episode and observe \n",
        "      self.state                  = env.reset()\n",
        "      \n",
        "      #Remember that stack frame function also call our preprocess function\n",
        "      self.state, self.stacked_frames  = stack_frames(\n",
        "          self.stacked_frames, \n",
        "          self.state, \n",
        "          is_new_episode=True\n",
        "      )\n",
        "      \n",
        "      while self.step < self.max_steps:\n",
        "        self.step       += 1\n",
        "        \n",
        "        #Increase the decay step\n",
        "        self.decay_step += 1\n",
        "        \n",
        "        #Predict the action to take and take it\n",
        "        self.action, self.explore_probability, self.exploration_vs_explotation = self.predict_action(\n",
        "            self.decay_step,\n",
        "            self.state\n",
        "        )\n",
        "        \n",
        "        #Perform the action and get the next_state, reward, and done info\n",
        "        self.next_state, self.reward, self.done, _ = env.step(np.argmax(self.action))\n",
        "        self.episode_actions.append(np.argmax(self.action))\n",
        "        \n",
        "        if episode_render:\n",
        "          env.render()\n",
        "          \n",
        "        #Add the reward to the total reward\n",
        "        self.episode_rewards.append(self.reward)\n",
        "        \n",
        "        if self.done:\n",
        "          #The episode ends so no next state\n",
        "          self.next_state = np.zeros( (110, 84) , dtype=np.int )\n",
        "          \n",
        "          self.next_state, self.stacked_frames = stack_frames(\n",
        "              self.stacked_frames, \n",
        "              self.next_state,\n",
        "              is_new_episode=False\n",
        "          )\n",
        "          \n",
        "          #Set Step = max_steps to end the episode\n",
        "          self.step = self.max_steps\n",
        "          \n",
        "          #Get the total reward of the episode\n",
        "          total_reward = np.sum(self.episode_rewards)\n",
        "          total_loss   = np.sum(self.episode_loss)\n",
        "          \n",
        "          self.save_log(\n",
        "              episode,\n",
        "              total_reward,\n",
        "              self.explore_probability,\n",
        "              total_loss / self.step ,\n",
        "              total_loss , \n",
        "              ( time.time() - self.start_time) / 60,\n",
        "              self.step,\n",
        "              \"Exploration\"  if (self.exploration_vs_explotation) else \"Explotation\",\n",
        "              \"Done:\",\n",
        "              self.episode_actions\n",
        "          )\n",
        "          \n",
        "          self.reward_list.append( (episode, total_reward) )\n",
        "          \n",
        "          #Store transition <st,at,rt+1,st+1> in memoryD\n",
        "          self.memory.add( \n",
        "              (self.state, self.action, self.reward, self.next_state, self.done)\n",
        "          )\n",
        "          \n",
        "        else:\n",
        "          #Stack the frame of the next_state\n",
        "          self.next_state, stacked_frames = stack_frames(\n",
        "            self.stacked_frames, \n",
        "            self.next_state,\n",
        "            is_new_episode=False\n",
        "          )\n",
        "          \n",
        "          #Add experence to memory\n",
        "          self.memory.add( \n",
        "              (self.state, self.action, self.reward, self.next_state, self.done)\n",
        "          )\n",
        "          \n",
        "          #st+1 is now our current state\n",
        "          self.state = self.next_state\n",
        "          \n",
        "        ###LEARNING\n",
        "        #obtain random mini-batch from memory\n",
        "        if self.memory.len() > self.batch_size:\n",
        "          \n",
        "          batch              = self.memory.sample(self.batch_size)\n",
        "          states_mb          = T.FloatTensor(np.array([each[0] for each in batch])).to(self.Q_eval.device)\n",
        "          #actions_mb         = np.array([each[1] for each in batch])\n",
        "          rewards_mb         = T.FloatTensor(np.array([each[2] for each in batch])).to(self.Q_eval.device)\n",
        "          next_states_mb     = T.FloatTensor(np.array([each[3] for each in batch])).to(self.Q_eval.device)\n",
        "          dones_mb           = np.array([each[4] for each in batch])\n",
        "\n",
        "          target_Qs_batch    = []\n",
        "          \n",
        "          #Get Q values for next state\n",
        "          self.Qs_next_state       = self.Q_target.forward(next_states_mb)\n",
        "          self.Qs_state            = self.Q_eval.forward(states_mb)\n",
        "#           self.Qtarget             = T.zeros_like(self.Qs_next_state) \n",
        "          self.Qtarget             = self.Qs_state.clone() \n",
        "         \n",
        "          #Set Q_target = r if the episode ends at s+1, otherwise\n",
        "          #set Q_target = r + gamma*maxQ(s', a')\n",
        "          for i in range  (0, len(batch)):\n",
        "            terminal                    = dones_mb[i]\n",
        "            maxAction                   = T.argmax(self.Qs_next_state[i]).to(self.Q_eval.device)\n",
        "            #if we are in a terminal state only equals reward\n",
        "            if terminal:\n",
        "              #print(\"Row {} Col {} Row Val {}, Col Val {}\".format( i,maxAction, self.Qtarget[i,maxAction].item() , rewards_mb[i].item() ) )\n",
        "              self.Qtarget[i,maxAction] = rewards_mb[i]\n",
        "              \n",
        "            else:\n",
        "              #print(\"Row {} Col {} Row Val {}, Col Val {}\".format( i,maxAction, self.Qtarget[i,maxAction].item() ,  (rewards_mb[i] + gamma * T.max(self.Qs_next_state[i])).item() ) )\n",
        "              self.Qtarget[i,maxAction] = rewards_mb[i] + gamma * T.max(self.Qs_next_state[i])\n",
        "              \n",
        "            target_Qs_batch.append(self.Qtarget[i,maxAction])\n",
        "            #shape (batch_size, num_actions) we want first dim\n",
        "            #We want the loss function to be zero for every action except the maxAction\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "          \n",
        "          #print(self.Qtarget,\"\\n\" ,self.Qs_state,\"\\n\" , (self.Qtarget - self.Qs_state) )\n",
        "          #return;\n",
        "          self.loss       = self.Q_eval.loss(\n",
        "              self.Qtarget, \n",
        "              self.Qs_state\n",
        "          ).to(self.Q_eval.device)\n",
        "          # Optimize the model\n",
        "          self.Q_eval.optim.zero_grad()\n",
        "          self.loss.backward()\n",
        "          self.Q_eval.optim.step()\n",
        "          self.episode_loss.append(self.loss.item())\n",
        "          \n",
        "          \n",
        "          #Get the total reward of the episode\n",
        "          total_reward = np.sum(self.episode_rewards)\n",
        "          total_loss   = np.sum(self.episode_loss)\n",
        "          \n",
        "#           if(self.step % 500 == 0):\n",
        "#             self.save_log(\n",
        "#                 episode,\n",
        "#                 total_reward,\n",
        "#                 self.explore_probability,\n",
        "#                 total_loss / 500 , \n",
        "#                 total_loss, \n",
        "#                 ( time.time() - self.start_time) / 60,\n",
        "#                 self.step,\n",
        "#                 \"Exploration\"  if (self.exploration_vs_explotation) else \"Explotation\",\n",
        "#                 \"Running:\",\n",
        "#                 self.episode_actions\n",
        "#             )\n",
        "            \n",
        "          \n",
        "#           print(self.Qtarget.shape,  self.Qs_state.shape, self.batch_size, states_mb.shape )\n",
        "#           return;\n",
        "\n",
        "        \n",
        "      # Save model every 5 episodes\n",
        "      if episode % target_update == 0:\n",
        "          T.save(self.Q_eval.state_dict(), drive_path+\"/model_deepq_pytorch.ckpt\" )\n",
        "          self.Q_target.load_state_dict(self.Q_eval.state_dict())\n",
        "          print(\"Saving Model and Updating Target\")\n",
        "          \n",
        "          \n",
        "  \n",
        "  def play(self):\n",
        "    state                  = env.reset()\n",
        "    totalScore             = 0\n",
        "    self.decay_step        = 0\n",
        "\n",
        "    for i_episode in range(self.total_episodes):\n",
        "      self.state = env.reset()\n",
        "\n",
        "      self.state, self.stacked_frames  = stack_frames(\n",
        "          self.stacked_frames, \n",
        "          self.state, \n",
        "          is_new_episode=True\n",
        "      )\n",
        "\n",
        "      for t in range(self.max_steps):\n",
        "        if episode_render:\n",
        "          env.render()\n",
        "        #print(self.state)\n",
        "        #action = env.action_space.sample()\n",
        "        action   =  self.Q_eval.forward(T.Tensor(self.state))\n",
        "        action   =  T.argmax(action).item()\n",
        "\n",
        "        self.next_state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            self.next_state = np.zeros( (110, 84) , dtype=np.int )\n",
        "            self.next_state, self.stacked_frames = stack_frames(\n",
        "                self.stacked_frames, \n",
        "                self.next_state,\n",
        "                is_new_episode=False\n",
        "            )\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "        else:\n",
        "          self.next_state, stacked_frames = stack_frames(\n",
        "            self.stacked_frames, \n",
        "            self.next_state,\n",
        "            is_new_episode=False\n",
        "          )\n",
        "          self.state = self.next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XIAVtz51_AL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train"
      ]
    },
    {
      "metadata": {
        "id": "E7dQgZoWY3zm",
        "colab_type": "code",
        "outputId": "9e1038a5-5afd-4c2b-8b8e-777be1a82a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5378
        }
      },
      "cell_type": "code",
      "source": [
        "agent = Agent(  \n",
        "    stack_size       = stack_size,\n",
        "    state_size       = state_size,\n",
        "    action_size      = action_size,\n",
        "    learning_rate    = learning_rate,\n",
        "    total_episodes   = total_episodes,\n",
        "    max_steps        = max_steps,\n",
        "    batch_size       = batch_size,\n",
        "    explore_start    = explore_start,\n",
        "    explore_stop     = explore_stop,\n",
        "    decay_rate       = decay_rate,\n",
        "    gamma            = gamma,\n",
        "    pretrain_length  = pretrain_length,\n",
        "    memory_size      = memory_size,\n",
        "    training         = training, \n",
        "    episode_render   = episode_render,\n",
        "    possible_actions = possible_actions,\n",
        "    target_update    = target_update\n",
        ")\n",
        "if training:\n",
        "  print(\"Start Training\")\n",
        "  agent.train()\n",
        "else:\n",
        "  print(\"Start Playing\")\n",
        "  agent.play()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading New Weights\n",
            "__________HYPERPARAMETERS__________\n",
            " \n",
            "stack_size         4 \n",
            "state_size         [110, 84, 4] \n",
            "action_size        6 \n",
            "learning_rate      0.00025 \n",
            "total_episodes     20000 \n",
            "max_steps          50000 \n",
            "batch_size         10 \n",
            "explore_start      1.0 \n",
            "explore_stop       0.01 \n",
            "decay_rate         1e-05 \n",
            "gamma              0.9 \n",
            "pretrain_length    10 \n",
            "memory_size        10000 \n",
            "training           True \n",
            "episode_render     False \n",
            "target_update      10 \n",
            "possible_actions\n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]] \n",
            "\n",
            "Start Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZkAAAFfCAYAAAAszcLlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzJIkSZIkSZKkQZHRDgAAAAdJREFUof0/uswKQTO5fjgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4ba496afd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Done: Episode: 0 Total reward: 180.0 Explore P: 0.9934 nTraining RunningLoss 515.1801 Loss 0.0103 Time Lapse 0.4559623122215271 Min. Step: 50000 Exploration Action 1\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 1 Total reward: 110.0 Explore P: 0.9873 nTraining RunningLoss 608.3512 Loss 0.0122 Time Lapse 0.8879059473673503 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 2 Total reward: 275.0 Explore P: 0.9771 nTraining RunningLoss 0.5713 Loss 0.0000 Time Lapse 1.5921385526657104 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 3 Total reward: 195.0 Explore P: 0.9686 nTraining RunningLoss 596.8394 Loss 0.0119 Time Lapse 2.172302011648814 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 4 Total reward: 30.0 Explore P: 0.9639 nTraining RunningLoss 1.2507 Loss 0.0000 Time Lapse 2.5021082202593488 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 5 Total reward: 25.0 Explore P: 0.9600 nTraining RunningLoss 0.1129 Loss 0.0000 Time Lapse 2.778920300801595 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 6 Total reward: 90.0 Explore P: 0.9539 nTraining RunningLoss 0.5748 Loss 0.0000 Time Lapse 3.2013784567515056 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 7 Total reward: 105.0 Explore P: 0.9490 nTraining RunningLoss 0.0807 Loss 0.0000 Time Lapse 3.550232005119324 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 8 Total reward: 245.0 Explore P: 0.9407 nTraining RunningLoss 0.0053 Loss 0.0000 Time Lapse 4.146397030353546 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 9 Total reward: 75.0 Explore P: 0.9357 nTraining RunningLoss 4.1263 Loss 0.0001 Time Lapse 4.509283820788066 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 10 Total reward: 120.0 Explore P: 0.9292 nTraining RunningLoss 26.3168 Loss 0.0005 Time Lapse 4.981546052296957 Min. Step: 50000 Exploration Action 3\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 11 Total reward: 105.0 Explore P: 0.9239 nTraining RunningLoss 91.3273 Loss 0.0018 Time Lapse 5.369132494926452 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 12 Total reward: 45.0 Explore P: 0.9204 nTraining RunningLoss 0.5183 Loss 0.0000 Time Lapse 5.624044855435689 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 13 Total reward: 45.0 Explore P: 0.9168 nTraining RunningLoss 0.7497 Loss 0.0000 Time Lapse 5.894160755475363 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 14 Total reward: 180.0 Explore P: 0.9102 nTraining RunningLoss 0.1534 Loss 0.0000 Time Lapse 6.378521756331126 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 15 Total reward: 440.0 Explore P: 0.8987 nTraining RunningLoss 0.1454 Loss 0.0000 Time Lapse 7.236286560694377 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 16 Total reward: 75.0 Explore P: 0.8935 nTraining RunningLoss 0.0019 Loss 0.0000 Time Lapse 7.625318737824758 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 17 Total reward: 380.0 Explore P: 0.8845 nTraining RunningLoss 6.8805 Loss 0.0001 Time Lapse 8.303101714452108 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 18 Total reward: 185.0 Explore P: 0.8762 nTraining RunningLoss 165.8664 Loss 0.0033 Time Lapse 8.936407872041066 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 19 Total reward: 65.0 Explore P: 0.8716 nTraining RunningLoss 0.1729 Loss 0.0000 Time Lapse 9.290834534168244 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 20 Total reward: 35.0 Explore P: 0.8647 nTraining RunningLoss 8.0764 Loss 0.0002 Time Lapse 9.823130162556966 Min. Step: 50000 Exploration Action 5\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 21 Total reward: 45.0 Explore P: 0.8596 nTraining RunningLoss 0.0504 Loss 0.0000 Time Lapse 10.226558470726014 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 22 Total reward: 135.0 Explore P: 0.8539 nTraining RunningLoss 0.1864 Loss 0.0000 Time Lapse 10.688008387883505 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 23 Total reward: 515.0 Explore P: 0.8435 nTraining RunningLoss 0.0322 Loss 0.0000 Time Lapse 11.514689469337464 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 24 Total reward: 205.0 Explore P: 0.8359 nTraining RunningLoss 2.3105 Loss 0.0000 Time Lapse 12.127513027191162 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 25 Total reward: 350.0 Explore P: 0.8230 nTraining RunningLoss 0.1074 Loss 0.0000 Time Lapse 13.172747619946797 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 26 Total reward: 60.0 Explore P: 0.8198 nTraining RunningLoss 0.2351 Loss 0.0000 Time Lapse 13.441236980756123 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 27 Total reward: 20.0 Explore P: 0.8166 nTraining RunningLoss 0.8150 Loss 0.0000 Time Lapse 13.701452843348186 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 28 Total reward: 235.0 Explore P: 0.8097 nTraining RunningLoss 0.7818 Loss 0.0000 Time Lapse 14.278012951215109 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 29 Total reward: 15.0 Explore P: 0.8058 nTraining RunningLoss 0.0000 Loss 0.0000 Time Lapse 14.596168831984203 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 30 Total reward: 270.0 Explore P: 0.7982 nTraining RunningLoss 12.5215 Loss 0.0003 Time Lapse 15.228121121724447 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 31 Total reward: 185.0 Explore P: 0.7880 nTraining RunningLoss 98.5347 Loss 0.0020 Time Lapse 16.086727333068847 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 32 Total reward: 160.0 Explore P: 0.7837 nTraining RunningLoss 398.3552 Loss 0.0080 Time Lapse 16.448069842656455 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 33 Total reward: 120.0 Explore P: 0.7790 nTraining RunningLoss 1.3893 Loss 0.0000 Time Lapse 16.854130812486012 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 34 Total reward: 90.0 Explore P: 0.7758 nTraining RunningLoss 0.2639 Loss 0.0000 Time Lapse 17.12297885020574 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 35 Total reward: 180.0 Explore P: 0.7708 nTraining RunningLoss 0.0684 Loss 0.0000 Time Lapse 17.554651951789857 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 36 Total reward: 105.0 Explore P: 0.7666 nTraining RunningLoss 1.6272 Loss 0.0000 Time Lapse 17.921211338043214 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 37 Total reward: 110.0 Explore P: 0.7614 nTraining RunningLoss 4.3600 Loss 0.0001 Time Lapse 18.374239893754325 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 38 Total reward: 455.0 Explore P: 0.7545 nTraining RunningLoss 1.0851 Loss 0.0000 Time Lapse 18.98184626897176 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 39 Total reward: 425.0 Explore P: 0.7424 nTraining RunningLoss 4.1336 Loss 0.0001 Time Lapse 20.05935493707657 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 40 Total reward: 30.0 Explore P: 0.7388 nTraining RunningLoss 0.0187 Loss 0.0000 Time Lapse 20.386973698933918 Min. Step: 50000 Exploration Action 4\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 41 Total reward: 355.0 Explore P: 0.7330 nTraining RunningLoss 0.6350 Loss 0.0000 Time Lapse 20.91543975273768 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 42 Total reward: 150.0 Explore P: 0.7274 nTraining RunningLoss 1.1271 Loss 0.0000 Time Lapse 21.42486513853073 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 43 Total reward: 135.0 Explore P: 0.7221 nTraining RunningLoss 6.4437 Loss 0.0001 Time Lapse 21.916293712457023 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 44 Total reward: 60.0 Explore P: 0.7177 nTraining RunningLoss 4.1311 Loss 0.0001 Time Lapse 22.324438325564067 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 45 Total reward: 135.0 Explore P: 0.7124 nTraining RunningLoss 0.2551 Loss 0.0000 Time Lapse 22.82097511291504 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 46 Total reward: 30.0 Explore P: 0.7095 nTraining RunningLoss 0.0240 Loss 0.0000 Time Lapse 23.085195521513622 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 47 Total reward: 35.0 Explore P: 0.7064 nTraining RunningLoss 0.9659 Loss 0.0000 Time Lapse 23.37526108821233 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 48 Total reward: 55.0 Explore P: 0.7030 nTraining RunningLoss 625.9106 Loss 0.0125 Time Lapse 23.696570563316346 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 49 Total reward: 285.0 Explore P: 0.6962 nTraining RunningLoss 0.0342 Loss 0.0000 Time Lapse 24.331060445308687 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 50 Total reward: 140.0 Explore P: 0.6918 nTraining RunningLoss 0.0016 Loss 0.0000 Time Lapse 24.748197376728058 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 51 Total reward: 155.0 Explore P: 0.6882 nTraining RunningLoss 0.0178 Loss 0.0000 Time Lapse 25.09576994975408 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 52 Total reward: 105.0 Explore P: 0.6844 nTraining RunningLoss 0.0506 Loss 0.0000 Time Lapse 25.477765854199728 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 53 Total reward: 225.0 Explore P: 0.6774 nTraining RunningLoss 0.0579 Loss 0.0000 Time Lapse 26.176188910007475 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 54 Total reward: 140.0 Explore P: 0.6733 nTraining RunningLoss 0.2726 Loss 0.0000 Time Lapse 26.590236914157867 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 55 Total reward: 110.0 Explore P: 0.6677 nTraining RunningLoss 7.1580 Loss 0.0001 Time Lapse 27.160576355457305 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 56 Total reward: 240.0 Explore P: 0.6579 nTraining RunningLoss 0.8255 Loss 0.0000 Time Lapse 28.144779864947 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 57 Total reward: 75.0 Explore P: 0.6546 nTraining RunningLoss 0.4454 Loss 0.0000 Time Lapse 28.47851049900055 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 58 Total reward: 135.0 Explore P: 0.6511 nTraining RunningLoss 0.2637 Loss 0.0000 Time Lapse 28.837927798430126 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 59 Total reward: 180.0 Explore P: 0.6465 nTraining RunningLoss 0.8044 Loss 0.0000 Time Lapse 29.315509331226348 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 60 Total reward: 185.0 Explore P: 0.6401 nTraining RunningLoss 5.9370 Loss 0.0001 Time Lapse 29.983738446235655 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 61 Total reward: 150.0 Explore P: 0.6354 nTraining RunningLoss 0.0340 Loss 0.0000 Time Lapse 30.474793287118278 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 62 Total reward: 75.0 Explore P: 0.6310 nTraining RunningLoss 0.0197 Loss 0.0000 Time Lapse 30.938172407944997 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 63 Total reward: 105.0 Explore P: 0.6268 nTraining RunningLoss 0.7153 Loss 0.0000 Time Lapse 31.389496835072837 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 64 Total reward: 35.0 Explore P: 0.6234 nTraining RunningLoss 12.2367 Loss 0.0002 Time Lapse 31.754731965065 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 65 Total reward: 90.0 Explore P: 0.6203 nTraining RunningLoss 871.6484 Loss 0.0174 Time Lapse 32.08220599492391 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 66 Total reward: 145.0 Explore P: 0.6172 nTraining RunningLoss 0.0002 Loss 0.0000 Time Lapse 32.4146902124087 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 67 Total reward: 75.0 Explore P: 0.6131 nTraining RunningLoss 0.0042 Loss 0.0000 Time Lapse 32.86928922335307 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 68 Total reward: 210.0 Explore P: 0.6090 nTraining RunningLoss 0.0238 Loss 0.0000 Time Lapse 33.312012338638304 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 69 Total reward: 75.0 Explore P: 0.6067 nTraining RunningLoss 0.0408 Loss 0.0000 Time Lapse 33.57021367947261 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 70 Total reward: 340.0 Explore P: 0.6022 nTraining RunningLoss 0.9770 Loss 0.0000 Time Lapse 34.08304534753164 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 71 Total reward: 125.0 Explore P: 0.5988 nTraining RunningLoss 0.0866 Loss 0.0000 Time Lapse 34.4603670279185 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 72 Total reward: 180.0 Explore P: 0.5947 nTraining RunningLoss 0.1111 Loss 0.0000 Time Lapse 34.92335184415182 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 73 Total reward: 105.0 Explore P: 0.5908 nTraining RunningLoss 0.0860 Loss 0.0000 Time Lapse 35.37802994648616 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 74 Total reward: 55.0 Explore P: 0.5885 nTraining RunningLoss 1.7632 Loss 0.0000 Time Lapse 35.64047467311223 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 75 Total reward: 50.0 Explore P: 0.5851 nTraining RunningLoss 2.8813 Loss 0.0001 Time Lapse 36.02881644169489 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 76 Total reward: 20.0 Explore P: 0.5827 nTraining RunningLoss 0.3182 Loss 0.0000 Time Lapse 36.303535628318784 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 77 Total reward: 545.0 Explore P: 0.5750 nTraining RunningLoss 2.3639 Loss 0.0000 Time Lapse 37.201673873265584 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 78 Total reward: 55.0 Explore P: 0.5722 nTraining RunningLoss 0.7665 Loss 0.0000 Time Lapse 37.539263192812605 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 79 Total reward: 210.0 Explore P: 0.5681 nTraining RunningLoss 99.5314 Loss 0.0020 Time Lapse 38.01828644673029 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 80 Total reward: 130.0 Explore P: 0.5637 nTraining RunningLoss 10.3378 Loss 0.0002 Time Lapse 38.5497661391894 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 81 Total reward: 30.0 Explore P: 0.5610 nTraining RunningLoss 0.6490 Loss 0.0000 Time Lapse 38.874401907126106 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 82 Total reward: 105.0 Explore P: 0.5580 nTraining RunningLoss 226.3576 Loss 0.0045 Time Lapse 39.24569898049037 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 83 Total reward: 150.0 Explore P: 0.5542 nTraining RunningLoss 2.3750 Loss 0.0000 Time Lapse 39.701998964945474 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 84 Total reward: 50.0 Explore P: 0.5513 nTraining RunningLoss 227.1633 Loss 0.0045 Time Lapse 40.063119347890215 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 85 Total reward: 155.0 Explore P: 0.5471 nTraining RunningLoss 1.1965 Loss 0.0000 Time Lapse 40.57814426819483 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 86 Total reward: 80.0 Explore P: 0.5449 nTraining RunningLoss 0.1747 Loss 0.0000 Time Lapse 40.84385122060776 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 87 Total reward: 215.0 Explore P: 0.5407 nTraining RunningLoss 1.1800 Loss 0.0000 Time Lapse 41.37972970803579 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 88 Total reward: 75.0 Explore P: 0.5374 nTraining RunningLoss 0.3531 Loss 0.0000 Time Lapse 41.80115434726079 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 89 Total reward: 55.0 Explore P: 0.5353 nTraining RunningLoss 0.0015 Loss 0.0000 Time Lapse 42.068714002768196 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 90 Total reward: 160.0 Explore P: 0.5310 nTraining RunningLoss 0.5143 Loss 0.0000 Time Lapse 42.61680206457774 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 91 Total reward: 120.0 Explore P: 0.5273 nTraining RunningLoss 0.0285 Loss 0.0000 Time Lapse 43.10274835030238 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 92 Total reward: 345.0 Explore P: 0.5202 nTraining RunningLoss 1.1129 Loss 0.0000 Time Lapse 44.03499032258988 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 93 Total reward: 90.0 Explore P: 0.5171 nTraining RunningLoss 384.4928 Loss 0.0077 Time Lapse 44.45167665084203 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 94 Total reward: 65.0 Explore P: 0.5138 nTraining RunningLoss 1.7382 Loss 0.0000 Time Lapse 44.89399716456731 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 95 Total reward: 135.0 Explore P: 0.5102 nTraining RunningLoss 0.4757 Loss 0.0000 Time Lapse 45.38436561822891 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 96 Total reward: 210.0 Explore P: 0.5062 nTraining RunningLoss 22.9087 Loss 0.0005 Time Lapse 45.93523752291997 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 97 Total reward: 135.0 Explore P: 0.5022 nTraining RunningLoss 23.0658 Loss 0.0005 Time Lapse 46.476832993825276 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 98 Total reward: 190.0 Explore P: 0.4970 nTraining RunningLoss 4.3614 Loss 0.0001 Time Lapse 47.19891398350398 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 99 Total reward: 180.0 Explore P: 0.4930 nTraining RunningLoss 19.1911 Loss 0.0004 Time Lapse 47.76202768484752 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 100 Total reward: 315.0 Explore P: 0.4887 nTraining RunningLoss 3.1687 Loss 0.0001 Time Lapse 48.3724798599879 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 101 Total reward: 105.0 Explore P: 0.4855 nTraining RunningLoss 11.3767 Loss 0.0002 Time Lapse 48.83117900689443 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 102 Total reward: 240.0 Explore P: 0.4809 nTraining RunningLoss 0.0875 Loss 0.0000 Time Lapse 49.50101052920024 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 103 Total reward: 110.0 Explore P: 0.4777 nTraining RunningLoss 0.0421 Loss 0.0000 Time Lapse 49.96233792304993 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 104 Total reward: 350.0 Explore P: 0.4722 nTraining RunningLoss 2.5570 Loss 0.0001 Time Lapse 50.76842595338822 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 105 Total reward: 170.0 Explore P: 0.4684 nTraining RunningLoss 0.5500 Loss 0.0000 Time Lapse 51.335806250572205 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 106 Total reward: 120.0 Explore P: 0.4651 nTraining RunningLoss 0.1910 Loss 0.0000 Time Lapse 51.836195182800296 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 107 Total reward: 65.0 Explore P: 0.4633 nTraining RunningLoss 106.5970 Loss 0.0021 Time Lapse 52.10242085456848 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 108 Total reward: 125.0 Explore P: 0.4601 nTraining RunningLoss 0.6028 Loss 0.0000 Time Lapse 52.57442954381307 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 109 Total reward: 105.0 Explore P: 0.4567 nTraining RunningLoss 1.2615 Loss 0.0000 Time Lapse 53.08485995531082 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 110 Total reward: 410.0 Explore P: 0.4533 nTraining RunningLoss 217.6848 Loss 0.0044 Time Lapse 53.607402797540026 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 111 Total reward: 210.0 Explore P: 0.4498 nTraining RunningLoss 0.0045 Loss 0.0000 Time Lapse 54.14926100969315 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 112 Total reward: 5.0 Explore P: 0.4472 nTraining RunningLoss 0.6329 Loss 0.0000 Time Lapse 54.54546733697256 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 113 Total reward: 615.0 Explore P: 0.4416 nTraining RunningLoss 0.3477 Loss 0.0000 Time Lapse 55.42625991900762 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 114 Total reward: 105.0 Explore P: 0.4386 nTraining RunningLoss 3.0044 Loss 0.0001 Time Lapse 55.91869472662608 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 115 Total reward: 110.0 Explore P: 0.4353 nTraining RunningLoss 0.0199 Loss 0.0000 Time Lapse 56.46254400809606 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 116 Total reward: 75.0 Explore P: 0.4330 nTraining RunningLoss 0.0327 Loss 0.0000 Time Lapse 56.83612824281057 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 117 Total reward: 165.0 Explore P: 0.4293 nTraining RunningLoss 0.3174 Loss 0.0000 Time Lapse 57.457140640417734 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 118 Total reward: 690.0 Explore P: 0.4237 nTraining RunningLoss 11.5263 Loss 0.0002 Time Lapse 58.38727671305339 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 119 Total reward: 105.0 Explore P: 0.4218 nTraining RunningLoss 0.7750 Loss 0.0000 Time Lapse 58.71577458779017 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 120 Total reward: 240.0 Explore P: 0.4162 nTraining RunningLoss 0.0032 Loss 0.0000 Time Lapse 59.65647681951523 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 121 Total reward: 75.0 Explore P: 0.4136 nTraining RunningLoss 0.6229 Loss 0.0000 Time Lapse 60.102595639228824 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 122 Total reward: 120.0 Explore P: 0.4104 nTraining RunningLoss 0.0064 Loss 0.0000 Time Lapse 60.663193400700884 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 123 Total reward: 60.0 Explore P: 0.4088 nTraining RunningLoss 2.6383 Loss 0.0001 Time Lapse 60.92844475110372 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 124 Total reward: 105.0 Explore P: 0.4072 nTraining RunningLoss 4.0125 Loss 0.0001 Time Lapse 61.213514959812166 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 125 Total reward: 135.0 Explore P: 0.4042 nTraining RunningLoss 0.0445 Loss 0.0000 Time Lapse 61.71418841679891 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 126 Total reward: 80.0 Explore P: 0.4010 nTraining RunningLoss 4.1005 Loss 0.0001 Time Lapse 62.276201637585956 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 127 Total reward: 120.0 Explore P: 0.3977 nTraining RunningLoss 1.2481 Loss 0.0000 Time Lapse 62.83223847548167 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 128 Total reward: 155.0 Explore P: 0.3948 nTraining RunningLoss 1.0401 Loss 0.0000 Time Lapse 63.335728804270424 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 129 Total reward: 165.0 Explore P: 0.3915 nTraining RunningLoss 0.1941 Loss 0.0000 Time Lapse 63.903255661328636 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 130 Total reward: 255.0 Explore P: 0.3878 nTraining RunningLoss 0.0464 Loss 0.0000 Time Lapse 64.56142736673355 Min. Step: 50000 Exploration Action 1\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 131 Total reward: 20.0 Explore P: 0.3864 nTraining RunningLoss 0.0504 Loss 0.0000 Time Lapse 64.8164340297381 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 132 Total reward: 195.0 Explore P: 0.3827 nTraining RunningLoss 0.0092 Loss 0.0000 Time Lapse 65.46049753030141 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 133 Total reward: 75.0 Explore P: 0.3808 nTraining RunningLoss 0.0223 Loss 0.0000 Time Lapse 65.80143166780472 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 134 Total reward: 105.0 Explore P: 0.3784 nTraining RunningLoss 13.6428 Loss 0.0003 Time Lapse 66.23517571290334 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 135 Total reward: 110.0 Explore P: 0.3760 nTraining RunningLoss 2.0270 Loss 0.0000 Time Lapse 66.65443621079127 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 136 Total reward: 210.0 Explore P: 0.3730 nTraining RunningLoss 0.4177 Loss 0.0000 Time Lapse 67.22024339040121 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 137 Total reward: 145.0 Explore P: 0.3697 nTraining RunningLoss 0.4293 Loss 0.0000 Time Lapse 67.83339544932048 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 138 Total reward: 280.0 Explore P: 0.3659 nTraining RunningLoss 0.0073 Loss 0.0000 Time Lapse 68.56181724071503 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 139 Total reward: 210.0 Explore P: 0.3625 nTraining RunningLoss 0.1347 Loss 0.0000 Time Lapse 69.20595622460047 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 140 Total reward: 75.0 Explore P: 0.3604 nTraining RunningLoss 2.8335 Loss 0.0001 Time Lapse 69.62174753348033 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 141 Total reward: 140.0 Explore P: 0.3576 nTraining RunningLoss 1.7742 Loss 0.0000 Time Lapse 70.15371579329172 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 142 Total reward: 60.0 Explore P: 0.3555 nTraining RunningLoss 12.7835 Loss 0.0003 Time Lapse 70.56956711212794 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 143 Total reward: 65.0 Explore P: 0.3540 nTraining RunningLoss 0.1076 Loss 0.0000 Time Lapse 70.85429490009943 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 144 Total reward: 110.0 Explore P: 0.3514 nTraining RunningLoss 1.5925 Loss 0.0000 Time Lapse 71.37634563048681 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 145 Total reward: 105.0 Explore P: 0.3487 nTraining RunningLoss 22.8708 Loss 0.0005 Time Lapse 71.91035243272782 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 146 Total reward: 155.0 Explore P: 0.3460 nTraining RunningLoss 1.3483 Loss 0.0000 Time Lapse 72.44860191742579 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 147 Total reward: 135.0 Explore P: 0.3432 nTraining RunningLoss 2.1988 Loss 0.0000 Time Lapse 73.00343406995138 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 148 Total reward: 30.0 Explore P: 0.3415 nTraining RunningLoss 0.1040 Loss 0.0000 Time Lapse 73.34265813827514 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 149 Total reward: 105.0 Explore P: 0.3386 nTraining RunningLoss 6.2840 Loss 0.0001 Time Lapse 73.94569195906321 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 150 Total reward: 120.0 Explore P: 0.3362 nTraining RunningLoss 4.8772 Loss 0.0001 Time Lapse 74.44048502047856 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 151 Total reward: 80.0 Explore P: 0.3342 nTraining RunningLoss 3.2442 Loss 0.0001 Time Lapse 74.85232616265615 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 152 Total reward: 75.0 Explore P: 0.3326 nTraining RunningLoss 0.0012 Loss 0.0000 Time Lapse 75.18424746195475 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 153 Total reward: 30.0 Explore P: 0.3310 nTraining RunningLoss 0.3011 Loss 0.0000 Time Lapse 75.50800880591075 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 154 Total reward: 45.0 Explore P: 0.3297 nTraining RunningLoss 0.0760 Loss 0.0000 Time Lapse 75.77629815737406 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 155 Total reward: 180.0 Explore P: 0.3272 nTraining RunningLoss 0.2780 Loss 0.0000 Time Lapse 76.29368484020233 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 156 Total reward: 120.0 Explore P: 0.3251 nTraining RunningLoss 0.0843 Loss 0.0000 Time Lapse 76.73984583616257 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 157 Total reward: 120.0 Explore P: 0.3224 nTraining RunningLoss 0.9173 Loss 0.0000 Time Lapse 77.30110085010529 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 158 Total reward: 110.0 Explore P: 0.3201 nTraining RunningLoss 0.0592 Loss 0.0000 Time Lapse 77.8133665839831 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 159 Total reward: 110.0 Explore P: 0.3177 nTraining RunningLoss 0.0377 Loss 0.0000 Time Lapse 78.33817588090896 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 160 Total reward: 80.0 Explore P: 0.3166 nTraining RunningLoss 0.3270 Loss 0.0000 Time Lapse 78.60238083998362 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 161 Total reward: 50.0 Explore P: 0.3147 nTraining RunningLoss 0.0004 Loss 0.0000 Time Lapse 79.02506098349889 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 162 Total reward: 110.0 Explore P: 0.3127 nTraining RunningLoss 0.1987 Loss 0.0000 Time Lapse 79.45681351423264 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 163 Total reward: 50.0 Explore P: 0.3111 nTraining RunningLoss 1.6152 Loss 0.0000 Time Lapse 79.81788993676504 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 164 Total reward: 180.0 Explore P: 0.3085 nTraining RunningLoss 0.0227 Loss 0.0000 Time Lapse 80.39325609604518 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 165 Total reward: 0.0 Explore P: 0.3070 nTraining RunningLoss 0.8959 Loss 0.0000 Time Lapse 80.73423708279928 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 166 Total reward: 210.0 Explore P: 0.3049 nTraining RunningLoss 0.4899 Loss 0.0000 Time Lapse 81.23452581564585 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 167 Total reward: 370.0 Explore P: 0.3025 nTraining RunningLoss 0.4077 Loss 0.0000 Time Lapse 81.79322758118312 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 168 Total reward: 65.0 Explore P: 0.3006 nTraining RunningLoss 215.9960 Loss 0.0043 Time Lapse 82.25027928749721 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 169 Total reward: 50.0 Explore P: 0.2990 nTraining RunningLoss 0.0060 Loss 0.0000 Time Lapse 82.62017435232798 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 170 Total reward: 90.0 Explore P: 0.2972 nTraining RunningLoss 0.0164 Loss 0.0000 Time Lapse 83.0699171145757 Min. Step: 50000 Exploration Action 5\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 171 Total reward: 50.0 Explore P: 0.2957 nTraining RunningLoss 8.7529 Loss 0.0002 Time Lapse 83.42794883648554 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 172 Total reward: 95.0 Explore P: 0.2934 nTraining RunningLoss 0.0346 Loss 0.0000 Time Lapse 83.98889286120733 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 173 Total reward: 165.0 Explore P: 0.2912 nTraining RunningLoss 0.3181 Loss 0.0000 Time Lapse 84.53116969664892 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 174 Total reward: 20.0 Explore P: 0.2899 nTraining RunningLoss 0.1283 Loss 0.0000 Time Lapse 84.86508920590083 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 175 Total reward: 50.0 Explore P: 0.2880 nTraining RunningLoss 0.0001 Loss 0.0000 Time Lapse 85.30689836740494 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 176 Total reward: 50.0 Explore P: 0.2869 nTraining RunningLoss 229.5989 Loss 0.0046 Time Lapse 85.57915015618006 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 177 Total reward: 30.0 Explore P: 0.2858 nTraining RunningLoss 8.4529 Loss 0.0002 Time Lapse 85.8579383055369 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 178 Total reward: 10.0 Explore P: 0.2842 nTraining RunningLoss 0.0491 Loss 0.0000 Time Lapse 86.2339815100034 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 179 Total reward: 260.0 Explore P: 0.2814 nTraining RunningLoss 0.0064 Loss 0.0000 Time Lapse 86.927843050162 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 180 Total reward: 65.0 Explore P: 0.2790 nTraining RunningLoss 1.8231 Loss 0.0000 Time Lapse 87.50695294936499 Min. Step: 50000 Exploration Action 4\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 181 Total reward: 50.0 Explore P: 0.2775 nTraining RunningLoss 25.6135 Loss 0.0005 Time Lapse 87.90962483882905 Min. Step: 50000 Exploration Action 2\n",
            "Done: Episode: 182 Total reward: 80.0 Explore P: 0.2764 nTraining RunningLoss 1.1987 Loss 0.0000 Time Lapse 88.16321207682292 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 183 Total reward: 120.0 Explore P: 0.2744 nTraining RunningLoss 0.1944 Loss 0.0000 Time Lapse 88.68076054255168 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 184 Total reward: 30.0 Explore P: 0.2726 nTraining RunningLoss 222.5377 Loss 0.0045 Time Lapse 89.1201600710551 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 185 Total reward: 135.0 Explore P: 0.2705 nTraining RunningLoss 5.6674 Loss 0.0001 Time Lapse 89.64988272190094 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 186 Total reward: 105.0 Explore P: 0.2688 nTraining RunningLoss 0.8789 Loss 0.0000 Time Lapse 90.09828399817148 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 187 Total reward: 75.0 Explore P: 0.2675 nTraining RunningLoss 0.4618 Loss 0.0000 Time Lapse 90.44038686752319 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 188 Total reward: 240.0 Explore P: 0.2650 nTraining RunningLoss 2.1698 Loss 0.0000 Time Lapse 91.09131562312444 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 189 Total reward: 140.0 Explore P: 0.2626 nTraining RunningLoss 1.7834 Loss 0.0000 Time Lapse 91.7257137576739 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 190 Total reward: 75.0 Explore P: 0.2610 nTraining RunningLoss 0.3754 Loss 0.0000 Time Lapse 92.17410608927409 Min. Step: 50000 Exploration Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 191 Total reward: 80.0 Explore P: 0.2589 nTraining RunningLoss 221.4256 Loss 0.0044 Time Lapse 92.72805149157843 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 192 Total reward: 105.0 Explore P: 0.2572 nTraining RunningLoss 0.4390 Loss 0.0000 Time Lapse 93.19583800633748 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 193 Total reward: 55.0 Explore P: 0.2557 nTraining RunningLoss 0.0020 Loss 0.0000 Time Lapse 93.61783707141876 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 194 Total reward: 215.0 Explore P: 0.2534 nTraining RunningLoss 0.3083 Loss 0.0000 Time Lapse 94.26485209465027 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 195 Total reward: 110.0 Explore P: 0.2517 nTraining RunningLoss 94.1580 Loss 0.0019 Time Lapse 94.71554171244303 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 196 Total reward: 50.0 Explore P: 0.2502 nTraining RunningLoss 0.1689 Loss 0.0000 Time Lapse 95.14785579045613 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 197 Total reward: 120.0 Explore P: 0.2484 nTraining RunningLoss 0.9381 Loss 0.0000 Time Lapse 95.67274857759476 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 198 Total reward: 5.0 Explore P: 0.2469 nTraining RunningLoss 1.3324 Loss 0.0000 Time Lapse 96.09006040096283 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 199 Total reward: 360.0 Explore P: 0.2447 nTraining RunningLoss 3.7759 Loss 0.0001 Time Lapse 96.71044729948044 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 200 Total reward: 105.0 Explore P: 0.2431 nTraining RunningLoss 1.2063 Loss 0.0000 Time Lapse 97.17127825419108 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 201 Total reward: 210.0 Explore P: 0.2413 nTraining RunningLoss 225.5714 Loss 0.0045 Time Lapse 97.71709910631179 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 202 Total reward: 30.0 Explore P: 0.2403 nTraining RunningLoss 0.0902 Loss 0.0000 Time Lapse 97.99225134849549 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 203 Total reward: 210.0 Explore P: 0.2384 nTraining RunningLoss 1.0001 Loss 0.0000 Time Lapse 98.55823026498159 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 204 Total reward: 15.0 Explore P: 0.2375 nTraining RunningLoss 0.8048 Loss 0.0000 Time Lapse 98.8220254500707 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 205 Total reward: 120.0 Explore P: 0.2361 nTraining RunningLoss 0.3018 Loss 0.0000 Time Lapse 99.25979882478714 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 206 Total reward: 30.0 Explore P: 0.2346 nTraining RunningLoss 2.3601 Loss 0.0000 Time Lapse 99.69738873243332 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 207 Total reward: 110.0 Explore P: 0.2329 nTraining RunningLoss 0.0221 Loss 0.0000 Time Lapse 100.19774755636851 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 208 Total reward: 75.0 Explore P: 0.2315 nTraining RunningLoss 5.2935 Loss 0.0001 Time Lapse 100.64952346483867 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 209 Total reward: 130.0 Explore P: 0.2298 nTraining RunningLoss 0.0049 Loss 0.0000 Time Lapse 101.16403883298238 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 210 Total reward: 75.0 Explore P: 0.2284 nTraining RunningLoss 0.0022 Loss 0.0000 Time Lapse 101.60476911067963 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 211 Total reward: 95.0 Explore P: 0.2260 nTraining RunningLoss 0.3084 Loss 0.0000 Time Lapse 102.34843337535858 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 212 Total reward: 15.0 Explore P: 0.2249 nTraining RunningLoss 0.9421 Loss 0.0000 Time Lapse 102.68829137881598 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 213 Total reward: 55.0 Explore P: 0.2235 nTraining RunningLoss 1.7949 Loss 0.0000 Time Lapse 103.14231315056483 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 214 Total reward: 110.0 Explore P: 0.2218 nTraining RunningLoss 0.0528 Loss 0.0000 Time Lapse 103.67737431526184 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 215 Total reward: 75.0 Explore P: 0.2203 nTraining RunningLoss 3.0093 Loss 0.0001 Time Lapse 104.14817397594452 Min. Step: 50000 Exploration Action 0\n",
            "Done: Episode: 216 Total reward: 135.0 Explore P: 0.2184 nTraining RunningLoss 95.2707 Loss 0.0019 Time Lapse 104.77811633745829 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 217 Total reward: 200.0 Explore P: 0.2164 nTraining RunningLoss 0.2574 Loss 0.0000 Time Lapse 105.42495131095251 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 218 Total reward: 105.0 Explore P: 0.2153 nTraining RunningLoss 2.1051 Loss 0.0000 Time Lapse 105.77625628709794 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 219 Total reward: 255.0 Explore P: 0.2123 nTraining RunningLoss 0.9180 Loss 0.0000 Time Lapse 106.76020797888438 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 220 Total reward: 105.0 Explore P: 0.2109 nTraining RunningLoss 0.1703 Loss 0.0000 Time Lapse 107.23295524915059 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 221 Total reward: 120.0 Explore P: 0.2093 nTraining RunningLoss 0.0061 Loss 0.0000 Time Lapse 107.79659208059311 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 222 Total reward: 75.0 Explore P: 0.2080 nTraining RunningLoss 0.0049 Loss 0.0000 Time Lapse 108.23355650901794 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 223 Total reward: 30.0 Explore P: 0.2067 nTraining RunningLoss 0.2977 Loss 0.0000 Time Lapse 108.67396582365036 Min. Step: 50000 Exploration Action 4\n",
            "Done: Episode: 224 Total reward: 105.0 Explore P: 0.2054 nTraining RunningLoss 0.2957 Loss 0.0000 Time Lapse 109.11510076522828 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 225 Total reward: 50.0 Explore P: 0.2042 nTraining RunningLoss 2.8420 Loss 0.0001 Time Lapse 109.55286054611206 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 226 Total reward: 120.0 Explore P: 0.2027 nTraining RunningLoss 0.0000 Loss 0.0000 Time Lapse 110.08959640264511 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 227 Total reward: 160.0 Explore P: 0.2008 nTraining RunningLoss 0.5312 Loss 0.0000 Time Lapse 110.75070077180862 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 228 Total reward: 185.0 Explore P: 0.1990 nTraining RunningLoss 29.5724 Loss 0.0006 Time Lapse 111.3828334848086 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 229 Total reward: 185.0 Explore P: 0.1966 nTraining RunningLoss 0.3881 Loss 0.0000 Time Lapse 112.25729101101557 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 230 Total reward: 50.0 Explore P: 0.1953 nTraining RunningLoss 6.9339 Loss 0.0001 Time Lapse 112.71750103632608 Min. Step: 50000 Exploration Action 2\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 231 Total reward: 50.0 Explore P: 0.1944 nTraining RunningLoss 3.3399 Loss 0.0001 Time Lapse 113.06587560574214 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 232 Total reward: 155.0 Explore P: 0.1926 nTraining RunningLoss 650.5536 Loss 0.0130 Time Lapse 113.73512990077337 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 233 Total reward: 210.0 Explore P: 0.1910 nTraining RunningLoss 0.3410 Loss 0.0000 Time Lapse 114.3285103281339 Min. Step: 50000 Exploration Action 1\n",
            "Done: Episode: 234 Total reward: 210.0 Explore P: 0.1893 nTraining RunningLoss 228.0111 Loss 0.0046 Time Lapse 114.95878895521165 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 235 Total reward: 75.0 Explore P: 0.1881 nTraining RunningLoss 0.0801 Loss 0.0000 Time Lapse 115.42139008839925 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 236 Total reward: 135.0 Explore P: 0.1857 nTraining RunningLoss 2.5267 Loss 0.0001 Time Lapse 116.33338053623835 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 237 Total reward: 45.0 Explore P: 0.1843 nTraining RunningLoss 389.3522 Loss 0.0078 Time Lapse 116.87960666020712 Min. Step: 50000 Exploration Action 5\n",
            "Done: Episode: 238 Total reward: 205.0 Explore P: 0.1827 nTraining RunningLoss 0.0066 Loss 0.0000 Time Lapse 117.52766589721044 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 239 Total reward: 215.0 Explore P: 0.1806 nTraining RunningLoss 0.3322 Loss 0.0000 Time Lapse 118.34352275530497 Min. Step: 50000 Exploration Action 3\n",
            "Done: Episode: 240 Total reward: 50.0 Explore P: 0.1795 nTraining RunningLoss 0.2512 Loss 0.0000 Time Lapse 118.77639158964158 Min. Step: 50000 Explotation Action 0\n",
            "Saving Model and Updating Target\n",
            "Done: Episode: 241 Total reward: 105.0 Explore P: 0.1784 nTraining RunningLoss 40.9921 Loss 0.0008 Time Lapse 119.22361634969711 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 242 Total reward: 45.0 Explore P: 0.1773 nTraining RunningLoss 1.0807 Loss 0.0000 Time Lapse 119.6978452205658 Min. Step: 50000 Explotation Action 0\n",
            "Done: Episode: 243 Total reward: 260.0 Explore P: 0.1752 nTraining RunningLoss 0.0225 Loss 0.0000 Time Lapse 120.56682300567627 Min. Step: 50000 Exploration Action 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}